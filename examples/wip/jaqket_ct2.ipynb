{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"intfloat/multilingual-e5-small\"\n",
    "\n",
    "WORKING_DIR = \"/home/hotchpotch/src/huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings/\"\n",
    "\n",
    "WIKIPEDIA_DS = \"singletongue/wikipedia-utils\"\n",
    "WIKIPEDIA_DS_NAME = \"passages-c400-jawiki-20230403\"\n",
    "# DS_NAME = 'hotchpotch/wikipedia-ja-20231030'\n",
    "\n",
    "INDEX_NAME = \"faiss_indexes/passages-c400-jawiki-20230403/multilingual-e5-small-passage/index_m96_mbit8_nlist512.faiss\"\n",
    "\n",
    "# INDEX_NAME = 'faiss_indexes/passages-c400-jawiki-20230403/multilingual-e5-small-passage/index_m8_mbit8_nlist512.faiss'\n",
    "\n",
    "\n",
    "# INDEX_NAME = 'faiss_indexes/passages-c400-jawiki-20230403/multilingual-e5-small-passage/index_flat_l2.faiss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = 'intfloat/multilingual-e5-large'\n",
    "\n",
    "# WORKING_DIR = '/home/hotchpotch/src/huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings/'\n",
    "\n",
    "# WIKIPEDIA_DS = 'singletongue/wikipedia-utils'\n",
    "# WIKIPEDIA_DS_NAME = 'passages-c400-jawiki-20230403'\n",
    "# # DS_NAME = 'hotchpotch/wikipedia-ja-20231030'\n",
    "# INDEX_NAME = 'faiss_indexes/passages-c400-jawiki-20230403/multilingual-e5-large-passage/index_m64_mbit8_nlist512.faiss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotchpotch/miniconda3/envs/llm-sc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets.download import DownloadManager\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(path=WIKIPEDIA_DS, name=WIKIPEDIA_DS_NAME, split=\"train\")\n",
    "# dm = DownloadManager()\n",
    "# index_pass  = dm.download(f\"https://huggingface.co/datasets/{DS_NAME}/resolve/main/{INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "jaqket_v1_dev_urls = [\n",
    "    \"https://jaqket.s3.ap-northeast-1.amazonaws.com/data/aio_01/dev1_questions.json\",\n",
    "    \"https://jaqket.s3.ap-northeast-1.amazonaws.com/data/aio_01/dev2_questions.json\",\n",
    "]\n",
    "\n",
    "\n",
    "# jaqket v1\n",
    "@dataclass\n",
    "class JaqketQuestionV1:\n",
    "    qid: str\n",
    "    question: str\n",
    "    answer_entity: str\n",
    "    label: int\n",
    "    answer_candidates: list[str]\n",
    "    original_question: str\n",
    "\n",
    "\n",
    "def load_jaqket_v1_dev(urls):\n",
    "    res = []\n",
    "    for url in urls:\n",
    "        with urllib.request.urlopen(url) as f:\n",
    "            # f は 1行ごとに処理\n",
    "            data = [json.loads(line.decode(\"utf-8\")) for line in f]\n",
    "        for d in data:\n",
    "            # label position\n",
    "            d[\"label\"] = d[\"answer_candidates\"].index(d[\"answer_entity\"])\n",
    "            # if -1\n",
    "            if d[\"label\"] == -1:\n",
    "                raise ValueError(\n",
    "                    f\"answer_entity not found in answer_candidates: {d['answer_entity']}, {d['answer_candidates']}\"\n",
    "                )\n",
    "            res.append(JaqketQuestionV1(**d))\n",
    "    return res\n",
    "\n",
    "\n",
    "jaqket_v1_dev = load_jaqket_v1_dev(jaqket_v1_dev_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hf_hub_ctranslate2 import CT2SentenceTransformer\n",
    "\n",
    "MODEL = CT2SentenceTransformer(\n",
    "    MODEL_NAME,\n",
    "    compute_type=\"int8_float16\",\n",
    "    device=\"cuda\",\n",
    ")\n",
    "MODEL.max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5555583"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.read_index(WORKING_DIR + INDEX_NAME)\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.nprobe = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "prefix = \"\"\n",
    "\n",
    "if \"-e5-\" in INDEX_NAME:\n",
    "    prefix = \"query: \"\n",
    "\n",
    "\n",
    "def texts_to_embs(texts, prefix=prefix) -> np.ndarray:\n",
    "    texts = [prefix + text for text in texts]\n",
    "    return MODEL.encode(texts, normalize_embeddings=True)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embs = texts_to_embs([q.question for q in jaqket_v1_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1992, 384)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search time: 7.370864152908325\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def faiss_search_by_embs(embs, faiss_index=index, top_k=5):\n",
    "    start_time = time.time()\n",
    "    D, I = faiss_index.search(embs, top_k)\n",
    "    end_time = time.time()\n",
    "    print(f\"search time: {end_time - start_time}\")\n",
    "    return D, I\n",
    "\n",
    "\n",
    "scores, indexes = faiss_search_by_embs(question_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_label_by_indexes(idxs, jaqket: JaqketQuestionV1, wiki_ds):\n",
    "    for idx in idxs:\n",
    "        data = wiki_ds[idx]\n",
    "        title = data[\"title\"]\n",
    "        # まずは title が jaqket の answer_candidates に完全一致するか\n",
    "        for j, candidate in enumerate(jaqket.answer_candidates):\n",
    "            if candidate == title:\n",
    "                return j\n",
    "        # XXX: RAG のユースケースを考えると、ここで続きも計算したほうが良い?\n",
    "\n",
    "    for idx in idxs:\n",
    "        data = wiki_ds[idx]\n",
    "        text = data[\"text\"]\n",
    "        # 次に text が jaqket の answer_candidates に含まれているか\n",
    "        for j, candidate in enumerate(jaqket.answer_candidates):\n",
    "            if candidate in text:\n",
    "                return j\n",
    "    return -1\n",
    "\n",
    "\n",
    "def predict_by_indexes(indexes, jaqket_ds, wiki_ds):\n",
    "    pred_labels = []\n",
    "    for idxs, jaqket in zip(indexes, jaqket_ds):\n",
    "        pred_label = find_label_by_indexes(idxs.tolist(), jaqket, wiki_ds)\n",
    "        pred_labels.append(pred_label)\n",
    "    return pred_labels\n",
    "\n",
    "\n",
    "pred_labels = predict_by_indexes(indexes, jaqket_v1_dev, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1179718875502008"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred labels に含まれる、-1 の割合\n",
    "sum([1 for l in pred_labels if l == -1]) / len(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [q.label for q in jaqket_v1_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6651606425702812"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正解率を表示\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(labels, pred_labels)\n",
    "\n",
    "# ct2 + float16\n",
    "# 0.6621485943775101\n",
    "# ct2 + int8_float16\n",
    "# 0.6651606425702812\n",
    "# ct2 + int8\n",
    "# 0.6651606425702812\n",
    "\n",
    "# k=5\n",
    "# 0.6731927710843374\n",
    "\n",
    "\n",
    "# k=5, m64\n",
    "# 0.6616465863453815\n",
    "\n",
    "# k=5, m48\n",
    "# 0.6616465863453815\n",
    "\n",
    "# k=5, m32\n",
    "# 0.588855421686747\n",
    "\n",
    "# k=5, m24\n",
    "# 0.588855421686747"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
