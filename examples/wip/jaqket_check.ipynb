{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"intfloat/multilingual-e5-small\"\n",
    "\n",
    "WORKING_DIR = \"/home/hotchpotch/src/huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings/\"\n",
    "\n",
    "WIKIPEDIA_DS = \"singletongue/wikipedia-utils\"\n",
    "WIKIPEDIA_DS_NAME = \"passages-c400-jawiki-20230403\"\n",
    "# DS_NAME = 'hotchpotch/wikipedia-ja-20231030'\n",
    "\n",
    "INDEX_NAME = \"faiss_indexes/passages-c400-jawiki-20230403/multilingual-e5-small-passage/index_m96_mbit8_nlist512.faiss\"\n",
    "\n",
    "# INDEX_NAME = 'faiss_indexes/passages-c400-jawiki-20230403/multilingual-e5-small-passage/index_m8_mbit8_nlist512.faiss'\n",
    "\n",
    "\n",
    "# INDEX_NAME = 'faiss_indexes/passages-c400-jawiki-20230403/multilingual-e5-small-passage/index_flat_l2.faiss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = 'intfloat/multilingual-e5-large'\n",
    "\n",
    "# WORKING_DIR = '/home/hotchpotch/src/huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings/'\n",
    "\n",
    "# WIKIPEDIA_DS = 'singletongue/wikipedia-utils'\n",
    "# WIKIPEDIA_DS_NAME = 'passages-c400-jawiki-20230403'\n",
    "# # DS_NAME = 'hotchpotch/wikipedia-ja-20231030'\n",
    "# INDEX_NAME = 'faiss_indexes/passages-c400-jawiki-20230403/multilingual-e5-large-passage/index_m64_mbit8_nlist512.faiss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hotchpotch/miniconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets.download import DownloadManager\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(path=WIKIPEDIA_DS, name=WIKIPEDIA_DS_NAME, split=\"train\")\n",
    "# dm = DownloadManager()\n",
    "# index_pass  = dm.download(f\"https://huggingface.co/datasets/{DS_NAME}/resolve/main/{INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "jaqket_v1_dev_urls = [\n",
    "    \"https://jaqket.s3.ap-northeast-1.amazonaws.com/data/aio_01/dev1_questions.json\",\n",
    "    \"https://jaqket.s3.ap-northeast-1.amazonaws.com/data/aio_01/dev2_questions.json\",\n",
    "]\n",
    "\n",
    "\n",
    "# jaqket v1\n",
    "@dataclass\n",
    "class JaqketQuestionV1:\n",
    "    qid: str\n",
    "    question: str\n",
    "    answer_entity: str\n",
    "    label: int\n",
    "    answer_candidates: list[str]\n",
    "    original_question: str\n",
    "\n",
    "\n",
    "def load_jaqket_v1_dev(urls):\n",
    "    res = []\n",
    "    for url in urls:\n",
    "        with urllib.request.urlopen(url) as f:\n",
    "            # f は 1行ごとに処理\n",
    "            data = [json.loads(line.decode(\"utf-8\")) for line in f]\n",
    "        for d in data:\n",
    "            # label position\n",
    "            d[\"label\"] = d[\"answer_candidates\"].index(d[\"answer_entity\"])\n",
    "            # if -1\n",
    "            if d[\"label\"] == -1:\n",
    "                raise ValueError(\n",
    "                    f\"answer_entity not found in answer_candidates: {d['answer_entity']}, {d['answer_candidates']}\"\n",
    "                )\n",
    "            res.append(JaqketQuestionV1(**d))\n",
    "    return res\n",
    "\n",
    "\n",
    "jaqket_v1_dev = load_jaqket_v1_dev(jaqket_v1_dev_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "最初はアンドレ・カンドレという芸名でデビューした、『リバーサイドホテル』『少年時代』などのヒット曲がある歌手は誰でしょう?\n",
      "次のキーワード一覧から答えよ。\n",
      "['徳永英明', '井上陽水', '鈴木雅之 (歌手)', '尾崎亜美', '南こうせつ', 'イルカ (歌手)', '竹内まりや', '松任谷由実', '小田和正', '小椋佳', '吉田拓郎', 'KAN', '渡辺美里', '佐野元春', '大江千里 (アーティスト)', '浜田省吾', '太田裕美', '財津和夫', '松任谷正隆', '中島みゆき']\n",
      "井上陽水\n",
      "---\n",
      "2014年には自身初の語録集『ぼちぼちしてらんねえ』を発表している、『乾杯』『とんぼ』などのヒット曲で知られる歌手は誰でしょう?\n",
      "次のキーワード一覧から答えよ。\n",
      "['沢田研二', '西城秀樹', '氷室京介', '吉川晃司', '尾崎豊', '長渕剛', '井上陽水', '松任谷由実', '近藤真彦', '布袋寅泰', '矢沢永吉', '佐野元春', '久保田利伸', '田原俊彦', '鈴木雅之 (歌手)', '小泉今日子', '忌野清志郎', '萩原健一', '世良公則', '浜田省吾']\n",
      "長渕剛\n",
      "---\n",
      "長男の裕哉も歌手として活躍している、『卒業』や『I LOVE YOU』といった楽曲のヒットにより80年代にカリスマ的な人気を博したミュージシャンは誰でしょう?\n",
      "次のキーワード一覧から答えよ。\n",
      "['徳永英明', '久保田利伸', 'KAN', '白井貴子 (歌手)', '井上陽水', '尾崎和行', '松任谷由実', '矢沢永吉', '小田和正', '大江千里 (アーティスト)', '渡辺美里', '吉川晃司', '中島みゆき', '浜田省吾', '吉田拓郎', '鈴木雅之 (歌手)', '佐野元春', '尾崎豊', 'Char', '稲垣潤一']\n",
      "尾崎豊\n",
      "---\n",
      "1975年に『アザミ嬢のララバイ』でデビューした女性歌手で、『わかれうた』『地上の星』などの曲を出しているのは誰?\n",
      "次のキーワード一覧から答えよ。\n",
      "['大貫妙子', '久保田利伸', '森山良子', '大江千里 (アーティスト)', '竹内まりや', '今井美樹', '渡辺美里', '松任谷由実', '鈴木雅之 (歌手)', '小田和正', '小林明子', '財津和夫', '井上陽水', '尾崎亜美', '山下達郎', '中島みゆき', '佐野元春', '松田聖子', '徳永英明', '南こうせつ']\n",
      "中島みゆき\n",
      "---\n",
      "玉置浩二がボーカルを務める、『悲しみにさよなら』や『ワインレッドの心』などの曲を歌った音楽グループは何?\n",
      "次のキーワード一覧から答えよ。\n",
      "['マスカレード/置き手紙', 'Make-up Shadow', 'I Love Youからはじめよう', 'メロディー (玉置浩二の曲)', 'SPY (槇原敬之の曲)', 'かなしみ笑い', '夏の終りのハーモニー/俺はシャウト!', '碧い瞳のエリス', '真夜中すぎの恋', '田園 (玉置浩二の曲)', '玉置浩二', '井上陽水', '熱視線', 'りばいばる', '月ひとしずく', '恋の予感', 'ワインレッドの心', '悲しみにさよなら', '恋人 (鈴木雅之の曲)', '安全地帯 (ロックバンド)']\n",
      "安全地帯 (ロックバンド)\n"
     ]
    }
   ],
   "source": [
    "for q in jaqket_v1_dev:\n",
    "    if \"井上陽水\" in q.answer_candidates:\n",
    "        print(\"---\")\n",
    "        print(f\"{q.question}\\n次のキーワード一覧から答えよ。\\n{q.answer_candidates}\")\n",
    "        print(q.answer_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MODEL = SentenceTransformer(MODEL_NAME, device=\"cuda\")\n",
    "MODEL.max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in faiss::FileIOReader::FileIOReader(const char *) at /Users/runner/work/faiss-wheels/faiss-wheels/faiss/faiss/impl/io.cpp:68: Error: 'f' failed: could not open /home/hotchpotch/src/huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings/faiss_indexes/passages-c400-jawiki-20230403/multilingual-e5-small-passage/index_m96_mbit8_nlist512.faiss for reading: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sc/d7y05bb97lz6qxwgj84rjl9c0000gn/T/ipykernel_4176/2474277187.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORKING_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mINDEX_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/faiss/swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   9923\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9924\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in faiss::FileIOReader::FileIOReader(const char *) at /Users/runner/work/faiss-wheels/faiss-wheels/faiss/faiss/impl/io.cpp:68: Error: 'f' failed: could not open /home/hotchpotch/src/huggingface.co/datasets/hotchpotch/wikipedia-passages-jawiki-embeddings/faiss_indexes/passages-c400-jawiki-20230403/multilingual-e5-small-passage/index_m96_mbit8_nlist512.faiss for reading: No such file or directory"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.read_index(WORKING_DIR + INDEX_NAME)\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.nprobe = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "prefix = \"\"\n",
    "\n",
    "if \"-e5-\" in INDEX_NAME:\n",
    "    prefix = \"query: \"\n",
    "\n",
    "\n",
    "def texts_to_embs(texts, prefix=prefix) -> np.ndarray:\n",
    "    texts = [prefix + text for text in texts]\n",
    "    return MODEL.encode(texts, normalize_embeddings=True)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embs = texts_to_embs([q.question for q in jaqket_v1_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1992, 384)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search time: 9.861521005630493\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def faiss_search_by_embs(embs, faiss_index=index, top_k=5):\n",
    "    start_time = time.time()\n",
    "    D, I = faiss_index.search(embs, top_k)\n",
    "    end_time = time.time()\n",
    "    print(f\"search time: {end_time - start_time}\")\n",
    "    return D, I\n",
    "\n",
    "\n",
    "scores, indexes = faiss_search_by_embs(question_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_label_by_indexes(idxs, jaqket: JaqketQuestionV1, wiki_ds):\n",
    "    for idx in idxs:\n",
    "        data = wiki_ds[idx]\n",
    "        title = data[\"title\"]\n",
    "        # まずは title が jaqket の answer_candidates に完全一致するか\n",
    "        for j, candidate in enumerate(jaqket.answer_candidates):\n",
    "            if candidate == title:\n",
    "                return j\n",
    "        # XXX: RAG のユースケースを考えると、ここで続きも計算したほうが良い?\n",
    "\n",
    "    for idx in idxs:\n",
    "        data = wiki_ds[idx]\n",
    "        text = data[\"text\"]\n",
    "        # 次に text が jaqket の answer_candidates に含まれているか\n",
    "        for j, candidate in enumerate(jaqket.answer_candidates):\n",
    "            if candidate in text:\n",
    "                return j\n",
    "    return -1\n",
    "\n",
    "\n",
    "def predict_by_indexes(indexes, jaqket_ds, wiki_ds):\n",
    "    pred_labels = []\n",
    "    for idxs, jaqket in zip(indexes, jaqket_ds):\n",
    "        pred_label = find_label_by_indexes(idxs.tolist(), jaqket, wiki_ds)\n",
    "        pred_labels.append(pred_label)\n",
    "    return pred_labels\n",
    "\n",
    "\n",
    "pred_labels = predict_by_indexes(indexes, jaqket_v1_dev, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11646586345381527"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred labels に含まれる、-1 の割合\n",
    "sum([1 for l in pred_labels if l == -1]) / len(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [q.label for q in jaqket_v1_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6616465863453815"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正解率を表示\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(labels, pred_labels)\n",
    "\n",
    "# k=5\n",
    "# 0.6731927710843374\n",
    "\n",
    "\n",
    "# k=5, m64\n",
    "# 0.6616465863453815\n",
    "\n",
    "# k=5, m48\n",
    "# 0.6616465863453815\n",
    "\n",
    "# k=5, m32\n",
    "# 0.588855421686747\n",
    "\n",
    "# k=5, m24\n",
    "# 0.588855421686747"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
